#!/usr/bin/env python3

"""Determine which endpoint provides the lowest latency for a set of probes.

Given a list of endpoint (attached to different upstream providers),
this program will tell which one has the lowest latency accross a set
of probes.

The set of probes can be chosen using various criteria and the results
can be grouped on various other criteria. The most interesting
criteria are the ASN and the country.

Credentials for RIPE Atlas API can be provided on the command-line or
through ``ATLAS_CREATE_KEY`` and ``ATLAS_DOWNLOAD_KEY`` environment
variables. The latest is only needed if results are not public (the
default).

"""

import os
import sys
import re
import time
import json
import logging
import argparse
import itertools
import functools
import collections
import statistics
import mmap
import csv
import requests
import math
import random

from ipaddress import ip_network, ip_address

import ripe.atlas.cousteau as atlas

WEEK = 60*60*24*7

logger = logging.getLogger("lowest-latency")


def parse():
    """Parse arguments."""
    parser = argparse.ArgumentParser(
        description=sys.modules[__name__].__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)

    g = parser.add_mutually_exclusive_group()
    g.add_argument("--debug", "-d", action="store_true",
                   default=False,
                   help="enable debugging")
    g.add_argument("--silent", "-s", action="store_true",
                   default=False,
                   help="don't log to console")

    g = parser.add_argument_group("Probe selection")
    g.add_argument("--limit", metavar="MAX", type=int, default=0,
                   help="Maximum number of probes to request")
    g.add_argument("--country", metavar="NAME", action="append",
                   help="Country code of probes")
    g.add_argument("--asn", metavar="ASN", action="append",
                   help="ASN of probes", type=int)
    g.add_argument("--prefix", metavar="PREFIX", type=ip_network,
                   action="append",
                   help="IP network prefix")
    g.add_argument("--city", metavar="NAME", action="append",
                   help="Limit probes to the given city")
    g.add_argument("--distance", metavar="KM", type=int,
                   default=50,
                   help="Maximum distance to specified cities")
    g.add_argument("--include-tag", metavar="TAG", type=str,
                   action="append",
                   help="Include only probes with this tag")
    g.add_argument("--exclude-tag", metavar="TAG", type=str,
                   action="append",
                   help="Exclude probes with this tag")
    g.add_argument("--no-cache", action="store_false", dest="cache",
                   help="Disable probe caching")

    g = parser.add_argument_group("Aggregation")
    g.add_argument("--aggregate", metavar="FIELD",
                   choices=("asn", "country", "city"),
                   action="append",
                   help="Aggregation criteria to use")
    g.add_argument("--aggregate-count", metavar="N",
                   type=int, default=4,
                   help="Sample number for the given aggregated population")

    g = parser.add_argument_group("Best endpoint selection")
    g.add_argument("--max-loss", metavar="PACKETS",
                   default=1,
                   help="Maximum acceptable packet loss")
    g.add_argument("--best-margin", metavar="PERCENT",
                   default=50, type=int,
                   help="Margin the best endpoint should have")

    g = parser.add_argument_group("Atlas API")
    g.add_argument("--api-create-key", metavar="KEY",
                   default=os.environ.get("ATLAS_CREATE_KEY", ""),
                   help="API key for create")
    g.add_argument("--api-download-key", metavar="KEY",
                   default=os.environ.get("ATLAS_DOWNLOAD_KEY", ""),
                   help="API key for download")
    g.add_argument("--msm", metavar="ID",
                   type=int, action="append",
                   help="Use the provided measurement IDs")
    g.add_argument("--public", action="store_true",
                   help="Make the results public")
    g.add_argument("--ping-count", metavar="COUNT",
                   type=int, default=3,
                   help="number of ping requests for each probe")

    parser.add_argument("--asn-database", metavar="PATH",
                        default="/usr/share/GeoIP/GeoIPASNum.dat",
                        help="Path to GeoIP ASN database")
    parser.add_argument("endpoint", metavar="IP", nargs="+",
                        type=ip_address,
                        help="endpoints to test")

    return parser.parse_args()


def get_family(options):
    """Determine the address family to use.

    Returns either 4 or 6. Will raise an exception in case of mixed
    family.
    """
    families = sorted([endpoint.version for endpoint in options.endpoint])
    if families[0] != families[-1]:
        raise ValueError("mixed IP families")
    logger.debug("IP family is {}".format(families[0]))
    return families[0]


def get_probes_for(options, family,
                   country=None,
                   asn=None, prefix=None):
    """Get a list of probes matching the provided options."""
    cachename = "probes-v{}".format(family)
    filters = {"tags": "system-ipv{}-works".format(family),
               "status": 1}
    if country:
        country = country.upper()
        filters["country_code"] = country
        cachename += "-country-{}".format(country)
    if asn:
        filters["asn_v{}".format(family)] = asn
        cachename += "-asn-{}".format(asn)
    if prefix:
        filters["prefix_v{}".format(family)] = prefix
        cachename += "-prefix-{}".format(prefix)
    cachename = re.sub(r'[^a-zA-Z0-9-]', '-', cachename)

    probes = None
    cached = False
    if options.cache:
        try:
            logger.debug("Check cache validity")
            with open(cachename) as cache:
                st = os.fstat(cache.fileno())
                if st.st_mtime + WEEK > time.time():
                    logger.debug(
                        "Retrieve cached list of probes from {}".format(
                            cachename))
                    try:
                        probes = json.load(cache)
                        cached = True
                    except ValueError as e:
                        logger.warn("Unable to read cache file {}: {}".format(
                            cachename, e))
                else:
                    logger.debug("Cache too old")
        except FileNotFoundError:
            logger.debug("Cache ({}) not available".format(cachename))
    else:
        logger.info("Cache disabled")

    if not probes:
        logger.info("Fetch list of probes (v{}, C: {}, AS: {}, P: {})".format(
            family, country or "any", asn or "any", prefix or "any"))
        probes = atlas.ProbeRequest(**filters)

    if options.cache and not cached:
        logger.debug("Write to cache {}".format(cachename))
        with open(cachename, "w") as cache:
            probes = list(probes)
            json.dump(probes, cache)

    return probes


def gcdistance(lat1, lon1, lat2, lon2):
    """Compute great circle distance between two points."""
    lon1, lat1, lon2, lat2 = (math.radians(x)
                              for x in [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = math.sin(dlat/2)**2 + \
        math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2
    c = 2 * math.asin(math.sqrt(a))
    km = 6367 * c
    return km


def filter_cities(options, probes):
    """Return a subset of the given probes matching the given cities.

    """
    if not options.city:
        return probes
    filtered = []
    for city in options.city:
        logger.debug("Getting coordinates for {}".format(city))
        r = requests.get("https://maps.googleapis.com/maps/api/geocode/json",
                         params=dict(sensor="false",
                                     address=city))
        r.raise_for_status()
        info = r.json()
        info = [i for i in info['results']
                if 'locality' in i['types']]
        info = info[0]['geometry']['location']
        logger.debug("City {} is latitude {} longitude {}".format(
            city, info["lat"], info["lng"]))
        found = [p for p in probes
                 if 'geometry' in p and p['geometry']['type'] == 'Point' and
                 gcdistance(p['geometry']['coordinates'][1],
                            p['geometry']['coordinates'][0],
                            info['lat'], info['lng']) < options.distance]
        for p in found:
            p['city'] = city
        logger.debug("Found {} probes near {}".format(len(found),
                                                      city))
        filtered.extend(found)
    return filtered


def filter_tags(options, probes):
    """Return a subset of the given probes matching the tag
    specification. This is done at this level to not mess with the
    caching mechanism.

    """
    include = set(options.include_tag or [])
    exclude = set(options.exclude_tag or [])
    filtered = [p for p in probes
                if include <= set(t['slug'] for t in p['tags'])
                and len(set(t['slug'] for t in p['tags']) & exclude) == 0]
    if len(filtered) != len(probes):
        logger.debug("Tag filter reduced the number of "
                     "probes from {} to {}".format(len(probes),
                                                   len(filtered)))
    return filtered

def get_probes(options, family):

    """Return a list of probes matching the provided options."""
    probes = []
    for country in (options.country or []):
        probes.extend(list(get_probes_for(options, family, country=country)))
    for asn in (options.asn or []):
        probes.extend(list(get_probes_for(options, family, asn=asn)))
    for prefix in (options.prefix or []):
        probes.extend(list(get_probes_for(options, family, prefix=prefix)))
    if not options.country and not options.asn and not options.prefix:
        probes = get_probes_for(options, family)
    probes = filter_tags(options, probes)
    probes = filter_cities(options, probes)
    if options.limit:
        random.shuffle(probes)
        probes = itertools.islice(probes, options.limit)
    probes = {probe['id']: probe for probe in probes}

    return probes.values()


def aggregate_key(options, family, probe):
    """Get the aggregation key for the given probe."""
    key = []
    for agg in (options.aggregate or []):
        if agg == "asn":
            key.append(str(probe["asn_v{}".format(family)]))
        if agg == "country":
            key.append(probe["country_code"])
        if agg == "city":
            key.append(probe["city"])
    key = "-".join(key) or "all"
    return key


def get_selected_probes(options, family, probes):
    """Select a subset of the available probes for aggregation purpose."""
    selected = collections.defaultdict(list)
    for probe in probes:
        key = aggregate_key(options, family, probe)
        selected[key].append(probe)
    probes = functools.reduce(lambda x, y: x+y,
                              [x[:options.aggregate_count]
                               for x in selected.values()
                               if len(x) >= options.aggregate_count],
                              [])
    for probe in probes:
        logger.debug("Select probe {} (ASN: {}, country: {})".format(
            probe["id"],
            probe["asn_v{}".format(family)],
            probe["country_code"]))
    logger.info("Selected {} probes".format(len(probes)))
    return probes


def send_measures(options, family, probes):
    """Get measures for the provided probes for the provided endpoints.

    We use a simple ping.
    """
    logger.debug("Send measure requests")
    measurements = []
    auth = {}
    if options.api_create_key:
        auth['key'] = options.api_create_key
    for endpoint in options.endpoint:
        measurements.append(atlas.Ping(
            af=family,
            target=str(endpoint),
            packets=options.ping_count,
            is_oneoff=True,
            is_public=options.public,
            description="lowest-latency ping test for {}".format(endpoint)))
    probes = list(probes)
    source = atlas.AtlasSource(type="probes",
                               requested=len(probes),
                               value=",".join([str(x['id']) for x in probes]))
    request = atlas.AtlasCreateRequest(sources=[source],
                                       measurements=measurements, **auth)
    success, response = request.create()
    if success:
        m = response['measurements']
        logger.info(
            "Measure requests successfully sent: {}".format(
                ", ".join([str(x) for x in m])))
        return m
    raise RuntimeError("Unable to send measure requests: {}".format(response))


def get_results(options, measures):
    """Fetch the given measures and return the results."""
    results = {}
    auth = {}
    if options.api_download_key:
        auth['key'] = options.api_download_key
    for msm in measures:
        logger.debug("Fetch metadata for {}".format(msm))
        count = 0
        while True:
            measurement = atlas.Measurement(id=msm, **auth)
            logger.debug("Current state for {}: {}".format(
                msm, measurement.status))
            if measurement.status == "Stopped":
                break
            count += 1
            if count % 10 == 0:
                logger.info(("Waiting for measurement {} "
                             "to complete "
                             "(current state: {})").format(
                                 msm, measurement.status))
            time.sleep(5)

        logger.debug("Fetch actual results for {}".format(msm))
        success, response = atlas.AtlasLatestRequest(msm_id=msm,
                                                     **auth).create()
        if success:
            logger.debug("Measure successfully fetched for {}".format(msm))
            results[msm] = response
        else:
            raise RuntimeError(
                "Unable to fetch results for measure {}: {}".format(
                    msm, response))
    return results


def get_summary(options, probes, results):
    """Get a summary of obtained results."""
    # Each result is something like this:
    # {'af': 4, 'avg': 75.1083666667,
    #  'dst_addr': 'xx.xx.xx.xx',
    #  'dst_name': 'xx.xx.xx.xx',
    #  'dup': 0,
    #  'from': '85.2.74.18',
    #  'fw': 4740,
    #  'group_id': 5444670,
    #  'lts': 111,
    #  'max': 80.328208,
    #  'min': 72.45467,
    #  'msm_id': 5444671,
    #  'msm_name': 'Ping',
    #  'prb_id': 956,
    #  'proto': 'ICMP',
    #  'rcvd': 3,
    #  'result': [{'rtt': 72.45467}, {'rtt': 80.328208}, {'rtt': 72.542222}],
    #  'sent': 3,
    #  'size': 48,
    #  'src_addr': '192.168.1.101',
    #  'step': 240,
    #  'timestamp': 1474442448,
    #  'ttl': 54,
    #  'type': 'ping'}

    # Turn the list of probes to a dict
    probes = {x['id']: x for x in probes}

    # Flatten the results
    results = functools.reduce(lambda x, y: x+y, results.values(), [])

    # Aggregate the results
    aggregated_results = collections.defaultdict(list)
    for result in results:
        probe = probes[result['prb_id']]
        key = aggregate_key(options, result['af'], probe)
        key = "{}-{}".format(result['dst_addr'], key)
        aggregated_results[key].append(result)

    # Reduction of results
    summary_results = {}
    for key in aggregated_results:
        rcvd = 0
        sent = 0
        for result in aggregated_results[key]:
            rcvd += result['rcvd']
            sent += result['sent']
        loss = (sent - rcvd) * 100. / sent
        if sent == 0:
            continue
        summary = dict(rcvd=rcvd,
                       sent=sent,
                       loss=loss)
        rtts = [statistics.median([y['rtt']
                                   for y in x['result']
                                   if 'rtt' in y])
                for x in aggregated_results[key]
                if x['rcvd'] > 0]
        if rtts:
            summary['mean'] = statistics.mean(rtts)
            summary['median'] = statistics.median(rtts)
            summary['stdev'] = len(rtts) > 1 and statistics.stdev(rtts) or 0
            summary['min'] = min(rtts)
            summary['max'] = max(rtts)
        summary_results[key] = summary
    return summary_results


def get_best_endpoint(options, key, summary_results):
    """Get the best endpoint for the given aggregation key."""
    max_loss = 100 * options.max_loss / 3 / options.aggregate_count + 1
    losses = False
    medians = {}
    for endpoint in options.endpoint:
        ekey = "{}-{}".format(endpoint, key)
        summary = summary_results[ekey]
        if summary["loss"] > max_loss:
            losses = True
            continue
        if 'median' not in summary:
            continue
        medians[endpoint] = summary['median']
    for endpoint in medians:
        # This endpoint is the best if it is better than any other and
        # it is better than one other by the given margin. If there
        # are losses, we have to select a endpoint.
        if medians[endpoint] != min(medians.values()):
            continue
        if losses or \
           medians[endpoint] * (1 + options.best_margin / 100.) <= max(
               medians.values()):
            return endpoint

    return None


@functools.lru_cache(maxsize=512)
def get_asn_description(database, asn):
    """Get the description for the given AS number.

    This is done by parsing a MaxMind legacy GeoIP ASN edition
    database and looking for chains starting with "AS" followed by
    the researched ASN. The idea is to keep the "parser" minimal.
    """
    try:
        with open(database, 'rb') as db:
            with mmap.mmap(db.fileno(), 0, access=mmap.ACCESS_READ) as buf:
                asb = b"\x00AS" + str(asn).encode() + b" "
                idx = buf.find(asb)
                if idx == -1:
                    return ""
                idx += len(asb)
                end = buf.find(b"\x00", idx)
                if end == -1:
                    # Odd?
                    return ""
                return buf[idx:end].decode(errors='replace')
    except FileNotFoundError:
        return ""


def display_summary(options, summary_results):
    """Display results in TSV format, group by endpoints."""
    csvout = csv.writer(sys.stdout, delimiter='\t')
    row = ['best', 'endpoint', 'key', 'loss', 'mean', 'median',
           'stdev', 'min', 'max']
    if options.aggregate == ['asn']:
        row.append('ASN')
    csvout.writerow(row)
    seen = {}
    for key in [key.split("-", 1)[1] for key in summary_results]:
        if key in seen:
            continue
        seen[key] = True
        if not all(["{}-{}".format(endpoint, key) in summary_results
                    for endpoint in options.endpoint]):
            continue
        isatty = os.isatty(sys.stdout.fileno())
        if isatty:
            best_endpoint = get_best_endpoint(options, key, summary_results)
        for endpoint in options.endpoint:
            ekey = "{}-{}".format(endpoint, key)
            summary = summary_results[ekey]
            row = [endpoint == best_endpoint and '*' or ' ',
                   endpoint, key,
                   summary.get('loss', ""),
                   summary.get('mean', ""),
                   summary.get('median', ""),
                   summary.get('stdev', ""),
                   summary.get('min', ""),
                   summary.get('max', "")]
            if options.aggregate == ['asn']:
                row.append(get_asn_description(options.asn_database, key))
            if isatty and endpoint == best_endpoint:
                print("\033[32m", end='')
            csvout.writerow([isinstance(x, float) and "{:.2f}".format(x) or x
                             for x in row])
            if isatty:
                print("\033[0m", end='')


if __name__ == "__main__":
    options = parse()

    # Logging
    logger.setLevel(options.debug and logging.DEBUG or
                    options.silent and logging.WARN or logging.INFO)
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter(
        "%(levelname)s[%(name)s] %(message)s"))
    logger.addHandler(ch)

    try:
        family = get_family(options)
        probes = get_probes(options, family)
        if not options.msm:
            selected_probes = get_selected_probes(options, family, probes)
            measures = send_measures(options, family, selected_probes)
        else:
            measures = options.msm
        results = get_results(options, measures)
        summary = get_summary(options, probes, results)
        display_summary(options, summary)
    except Exception as e:
        logger.exception("%s", e)
        sys.exit(1)
